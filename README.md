# Awesome_MOE
:rocket: The collections of MOE (Mixture Of Expert) papers, codes and tools, etc.

# Paper Lists
- [2024/01] Chinese-Mixtral-8x7B. [[github](https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B)]
- [2024/01] Sparse MoE with Language Guided Routing for Multilingual Machine Translation. [[paper](https://openreview.net/pdf?id=ySS7hH1smL)][[code](https://openreview.net/forum?id=ySS7hH1smL)]
- [2023/12] Mixtral 8x7B. [[blog](https://mistral.ai/news/mixtral-of-experts/)][[paper](https://arxiv.org/pdf/2401.04088.pdf)][[code](https://github.com/mistralai/mistral-src)]
- [2023/07] llama-moe： Building Mixture-of-Experts from LLaMA with Continual Pre-training [[paper](https://github.com/pjlab-sys4nlp/llama-moe/blob/main/docs/LLaMA_MoE.pdf)][[code](https://github.com/pjlab-sys4nlp/llama-moe)]
- [2023/05] Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models [[paper](https://arxiv.org/abs/2305.14705)]
- [2023/03] PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing. arXiv 2023. [[paper](https://arxiv.org/abs/2303.10845)]
- [2022] Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts. Findings of EMNLP 2022. [[paper](https://aclanthology.org/2022.findings-emnlp.189.pdf)]
- [2022/04] Sparsely activated mixture-of-experts are robust multi-task learners. [[paper](https://arxiv.org/pdf/2204.07689)]
- [2022/01] GLaM: Efficient Scaling of Language Models with Mixture-of-Experts. ICML 2022. [[paper](https://arxiv.org/abs/2112.06905)]
- [2022/01] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity[[paper](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf)]
- [2021] Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference. Findings of EMNLP 2021. [[paper](https://arxiv.org/abs/2110.03742)]
- [2020/06] GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding [[paper](https://arxiv.org/abs/2006.16668)]
- [2017/01] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer [[paper](https://arxiv.org/abs/1701.06538)]
