# Awesome_MOE
:rocket: The collections of MOE (Mixture Of Expert) papers, codes and tools, etc.

# Paper Lists
- [2024/03] Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM [[paper](https://arxiv.org/pdf/2403.07816.pdf)]
- [2024/01] Chinese-Mixtral-8x7B. [[github](https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B)]
- [2024/01] MoLE: Mixture of LoRA Experts [[paper](https://openreview.net/forum?id=uWvKBCYh4S)]
- [2024/01] Sparse MoE with Language Guided Routing for Multilingual Machine Translation. [[paper](https://openreview.net/pdf?id=ySS7hH1smL)][[code](https://openreview.net/forum?id=ySS7hH1smL)]
- [2024/01] Scalable Modular Network: A Framework for Adaptive Learning via Agreement Routing  [[paper](https://openreview.net/pdf?id=pEKJl5sflp)]
- [2023/12] Mixtral 8x7B. [[blog](https://mistral.ai/news/mixtral-of-experts/)][[paper](https://arxiv.org/pdf/2401.04088.pdf)][[code](https://github.com/mistralai/mistral-src)]
- [2023/10] Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy [[paper](https://arxiv.org/pdf/2310.01334.pdf)]
- [2023/10] Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures? [[paper](https://arxiv.org/pdf/2310.10908.pdf)]
- [2023/09] Large Language Model Routing with Benchmark Datasets [[paper](https://arxiv.org/pdf/2309.15789.pdf)]
- [2023/07] llama-moe： Building Mixture-of-Experts from LLaMA with Continual Pre-training [[paper](https://github.com/pjlab-sys4nlp/llama-moe/blob/main/docs/LLaMA_MoE.pdf)][[code](https://github.com/pjlab-sys4nlp/llama-moe)]
- [2023/05] Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models [[paper](https://arxiv.org/abs/2305.14705)]
- [2023/03] PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing. arXiv 2023. [[paper](https://arxiv.org/abs/2303.10845)]
- [2022] Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts. Findings of EMNLP 2022. [[paper](https://aclanthology.org/2022.findings-emnlp.189.pdf)]
- [2022/04] Sparsely activated mixture-of-experts are robust multi-task learners. [[paper](https://arxiv.org/pdf/2204.07689)]
- [2022/01] GLaM: Efficient Scaling of Language Models with Mixture-of-Experts. ICML 2022. [[paper](https://arxiv.org/abs/2112.06905)]
- [2022/01] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity[[paper](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf)]
- [2022] Spatial Mixture-of-Experts. [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/4c5e2bcbf21bdf40d75fddad0bd43dc9-Paper-Conference.pdf)]
- [2021/08] DEMix Layers: Disentangling Domains for Modular Language Modeling. [[paper](https://arxiv.org/abs/2108.05036)]
- [2021] Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference. Findings of EMNLP 2021. [[paper](https://arxiv.org/abs/2110.03742)]
- [2021] Scaling vision with sparse mixture of experts. [[paper](https://proceedings.neurips.cc/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf)]
- [2020/06] GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding [[paper](https://arxiv.org/abs/2006.16668)]
- [2018/01] Practical and theoretical aspects of mixture‐of‐experts modeling: An overview 
- [2017/01] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer [[paper](https://arxiv.org/abs/1701.06538)]
